{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Downloading https://files.pythonhosted.org/packages/72/5c/ec84c7ec49fde2c3b0d885ecae4504fa40fc77fef7684e9f2939c50f9b94/s3fs-0.4.0-py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/1f/7028dacd3c28f34ce48130aae73a88fa5cc27b6b0e494fcf2739f7954d9d/fsspec-0.6.2-py3-none-any.whl (62kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 4.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting boto3>=1.9.91 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/b5/3595b837d2aaf45b93adb8db44bb4ed07c04b3ce9ff6c399350314c779d2/boto3-1.11.8-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 9.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore>=1.12.91 (from s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/f3/f06005f90a09bbdd4bc6df76400f0ac279f7e1f556d635ab60fb1f916d1b/botocore-1.14.8-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 4.3MB/s eta 0:00:01    27% |████████▊                       | 1.6MB 25.5MB/s eta 0:00:01    71% |██████████████████████▉         | 4.2MB 29.3MB/s eta 0:00:01    96% |███████████████████████████████ | 5.7MB 33.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0 (from boto3>=1.9.91->s3fs)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/48/a8252b6b3cd31774eab312b19d58a6ac55f296240c206617dcd38cd93bf8/s3transfer-0.3.2-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 17.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.91->s3fs) (0.9.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.14)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.22)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.11.0)\n",
      "\u001b[31mawscli 1.16.17 has requirement botocore==1.12.7, but you'll have botocore 1.14.8 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.16.17 has requirement s3transfer<0.2.0,>=0.1.12, but you'll have s3transfer 0.3.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: fsspec, botocore, s3transfer, boto3, s3fs\n",
      "  Found existing installation: botocore 1.12.7\n",
      "    Uninstalling botocore-1.12.7:\n",
      "      Successfully uninstalled botocore-1.12.7\n",
      "  Found existing installation: s3transfer 0.1.13\n",
      "    Uninstalling s3transfer-0.1.13:\n",
      "      Successfully uninstalled s3transfer-0.1.13\n",
      "  Found existing installation: boto3 1.9.7\n",
      "    Uninstalling boto3-1.9.7:\n",
      "      Successfully uninstalled boto3-1.9.7\n",
      "Successfully installed boto3-1.11.8 botocore-1.14.8 fsspec-0.6.2 s3fs-0.4.0 s3transfer-0.3.2\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import boto3\n",
    "import pandas as pd\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\", region_name=\"us-west-2\", aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "                    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "searchobj = \"log_data\"\n",
    "lcobj = list(s3.list_objects_v2(Bucket=\"udacity-dend\", \n",
    "                                Prefix=searchobj + \"/\").values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdlist = list()\n",
    "prefix = 's3a://udacity-dend/'\n",
    "for o in lcobj[2]:\n",
    "    sdlist.append(prefix+o[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://udacity-dend/log_data/2018/11/2018-11-01-events.json'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdlist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(sdlist[1], lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 18)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdf = spark.read.json(sdlist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|        artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+--------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|          null|Logged In|   Walter|     M|            0|    Frye|     null| free|San Francisco-Oak...|   GET|    Home|1.540919166796E12|       38|                null|   200|1541105830796|\"Mozilla/5.0 (Mac...|    39|\n",
      "|          null|Logged In|   Kaylee|     F|            0| Summers|     null| free|Phoenix-Mesa-Scot...|   GET|    Home|1.540344794796E12|      139|                null|   200|1541106106796|\"Mozilla/5.0 (Win...|     8|\n",
      "|       Des'ree|Logged In|   Kaylee|     F|            1| Summers|246.30812| free|Phoenix-Mesa-Scot...|   PUT|NextSong|1.540344794796E12|      139|        You Gotta Be|   200|1541106106796|\"Mozilla/5.0 (Win...|     8|\n",
      "|          null|Logged In|   Kaylee|     F|            2| Summers|     null| free|Phoenix-Mesa-Scot...|   GET| Upgrade|1.540344794796E12|      139|                null|   200|1541106132796|\"Mozilla/5.0 (Win...|     8|\n",
      "|       Mr Oizo|Logged In|   Kaylee|     F|            3| Summers|144.03873| free|Phoenix-Mesa-Scot...|   PUT|NextSong|1.540344794796E12|      139|             Flat 55|   200|1541106352796|\"Mozilla/5.0 (Win...|     8|\n",
      "|    Tamba Trio|Logged In|   Kaylee|     F|            4| Summers|177.18812| free|Phoenix-Mesa-Scot...|   PUT|NextSong|1.540344794796E12|      139|Quem Quiser Encon...|   200|1541106496796|\"Mozilla/5.0 (Win...|     8|\n",
      "|The Mars Volta|Logged In|   Kaylee|     F|            5| Summers|380.42077| free|Phoenix-Mesa-Scot...|   PUT|NextSong|1.540344794796E12|      139|           Eriatarka|   200|1541106673796|\"Mozilla/5.0 (Win...|     8|\n",
      "+--------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'infer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Convert a JSON string to pandas object\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path_or_buf : a valid JSON string or file-like, default: None\n",
       "    The string could be a URL. Valid URL schemes include http, ftp, s3, and\n",
       "    file. For file URLs, a host is expected. For instance, a local file\n",
       "    could be ``file://localhost/path/to/table.json``\n",
       "\n",
       "orient : string,\n",
       "    Indication of expected JSON string format.\n",
       "    Compatible JSON strings can be produced by ``to_json()`` with a\n",
       "    corresponding orient value.\n",
       "    The set of possible orients is:\n",
       "\n",
       "    - ``'split'`` : dict like\n",
       "      ``{index -> [index], columns -> [columns], data -> [values]}``\n",
       "    - ``'records'`` : list like\n",
       "      ``[{column -> value}, ... , {column -> value}]``\n",
       "    - ``'index'`` : dict like ``{index -> {column -> value}}``\n",
       "    - ``'columns'`` : dict like ``{column -> {index -> value}}``\n",
       "    - ``'values'`` : just the values array\n",
       "\n",
       "    The allowed and default values depend on the value\n",
       "    of the `typ` parameter.\n",
       "\n",
       "    * when ``typ == 'series'``,\n",
       "\n",
       "      - allowed orients are ``{'split','records','index'}``\n",
       "      - default is ``'index'``\n",
       "      - The Series index must be unique for orient ``'index'``.\n",
       "\n",
       "    * when ``typ == 'frame'``,\n",
       "\n",
       "      - allowed orients are ``{'split','records','index',\n",
       "        'columns','values', 'table'}``\n",
       "      - default is ``'columns'``\n",
       "      - The DataFrame index must be unique for orients ``'index'`` and\n",
       "        ``'columns'``.\n",
       "      - The DataFrame columns must be unique for orients ``'index'``,\n",
       "        ``'columns'``, and ``'records'``.\n",
       "\n",
       "    .. versionadded:: 0.23.0\n",
       "       'table' as an allowed value for the ``orient`` argument\n",
       "\n",
       "typ : type of object to recover (series or frame), default 'frame'\n",
       "dtype : boolean or dict, default True\n",
       "    If True, infer dtypes, if a dict of column to dtype, then use those,\n",
       "    if False, then don't infer dtypes at all, applies only to the data.\n",
       "convert_axes : boolean, default True\n",
       "    Try to convert the axes to the proper dtypes.\n",
       "convert_dates : boolean, default True\n",
       "    List of columns to parse for dates; If True, then try to parse\n",
       "    datelike columns default is True; a column label is datelike if\n",
       "\n",
       "    * it ends with ``'_at'``,\n",
       "\n",
       "    * it ends with ``'_time'``,\n",
       "\n",
       "    * it begins with ``'timestamp'``,\n",
       "\n",
       "    * it is ``'modified'``, or\n",
       "\n",
       "    * it is ``'date'``\n",
       "\n",
       "keep_default_dates : boolean, default True\n",
       "    If parsing dates, then parse the default datelike columns\n",
       "numpy : boolean, default False\n",
       "    Direct decoding to numpy arrays. Supports numeric data only, but\n",
       "    non-numeric column and index labels are supported. Note also that the\n",
       "    JSON ordering MUST be the same for each term if numpy=True.\n",
       "precise_float : boolean, default False\n",
       "    Set to enable usage of higher precision (strtod) function when\n",
       "    decoding string to double values. Default (False) is to use fast but\n",
       "    less precise builtin functionality\n",
       "date_unit : string, default None\n",
       "    The timestamp unit to detect if converting dates. The default behaviour\n",
       "    is to try and detect the correct precision, but if this is not desired\n",
       "    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,\n",
       "    milliseconds, microseconds or nanoseconds respectively.\n",
       "lines : boolean, default False\n",
       "    Read the file as a json object per line.\n",
       "\n",
       "    .. versionadded:: 0.19.0\n",
       "\n",
       "encoding : str, default is 'utf-8'\n",
       "    The encoding to use to decode py3 bytes.\n",
       "\n",
       "    .. versionadded:: 0.19.0\n",
       "\n",
       "chunksize: integer, default None\n",
       "    Return JsonReader object for iteration.\n",
       "    See the `line-delimted json docs\n",
       "    <http://pandas.pydata.org/pandas-docs/stable/io.html#io-jsonl>`_\n",
       "    for more information on ``chunksize``.\n",
       "    This can only be passed if `lines=True`.\n",
       "    If this is None, the file will be read into memory all at once.\n",
       "\n",
       "    .. versionadded:: 0.21.0\n",
       "\n",
       "compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
       "    For on-the-fly decompression of on-disk data. If 'infer', then use\n",
       "    gzip, bz2, zip or xz if path_or_buf is a string ending in\n",
       "    '.gz', '.bz2', '.zip', or 'xz', respectively, and no decompression\n",
       "    otherwise. If using 'zip', the ZIP file must contain only one data\n",
       "    file to be read in. Set to None for no decompression.\n",
       "\n",
       "    .. versionadded:: 0.21.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "result : Series or DataFrame, depending on the value of `typ`.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Specific to ``orient='table'``, if a :class:`DataFrame` with a literal\n",
       ":class:`Index` name of `index` gets written with :func:`to_json`, the\n",
       "subsequent read operation will incorrectly set the :class:`Index` name to\n",
       "``None``. This is because `index` is also used by :func:`DataFrame.to_json`\n",
       "to denote a missing :class:`Index` name, and the subsequent\n",
       ":func:`read_json` operation cannot distinguish between the two. The same\n",
       "limitation is encountered with a :class:`MultiIndex` and any names\n",
       "beginning with ``'level_'``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DataFrame.to_json\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],\n",
       "...                   index=['row 1', 'row 2'],\n",
       "...                   columns=['col 1', 'col 2'])\n",
       "\n",
       "Encoding/decoding a Dataframe using ``'split'`` formatted JSON:\n",
       "\n",
       ">>> df.to_json(orient='split')\n",
       "'{\"columns\":[\"col 1\",\"col 2\"],\n",
       "  \"index\":[\"row 1\",\"row 2\"],\n",
       "  \"data\":[[\"a\",\"b\"],[\"c\",\"d\"]]}'\n",
       ">>> pd.read_json(_, orient='split')\n",
       "      col 1 col 2\n",
       "row 1     a     b\n",
       "row 2     c     d\n",
       "\n",
       "Encoding/decoding a Dataframe using ``'index'`` formatted JSON:\n",
       "\n",
       ">>> df.to_json(orient='index')\n",
       "'{\"row 1\":{\"col 1\":\"a\",\"col 2\":\"b\"},\"row 2\":{\"col 1\":\"c\",\"col 2\":\"d\"}}'\n",
       ">>> pd.read_json(_, orient='index')\n",
       "      col 1 col 2\n",
       "row 1     a     b\n",
       "row 2     c     d\n",
       "\n",
       "Encoding/decoding a Dataframe using ``'records'`` formatted JSON.\n",
       "Note that index labels are not preserved with this encoding.\n",
       "\n",
       ">>> df.to_json(orient='records')\n",
       "'[{\"col 1\":\"a\",\"col 2\":\"b\"},{\"col 1\":\"c\",\"col 2\":\"d\"}]'\n",
       ">>> pd.read_json(_, orient='records')\n",
       "  col 1 col 2\n",
       "0     a     b\n",
       "1     c     d\n",
       "\n",
       "Encoding with Table Schema\n",
       "\n",
       ">>> df.to_json(orient='table')\n",
       "'{\"schema\": {\"fields\": [{\"name\": \"index\", \"type\": \"string\"},\n",
       "                        {\"name\": \"col 1\", \"type\": \"string\"},\n",
       "                        {\"name\": \"col 2\", \"type\": \"string\"}],\n",
       "                \"primaryKey\": \"index\",\n",
       "                \"pandas_version\": \"0.20.0\"},\n",
       "    \"data\": [{\"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\"},\n",
       "            {\"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\"}]}'\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.6/site-packages/pandas/io/json/json.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pd.read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = \n",
    "    \n",
    "    # read song data file\n",
    "    df = \n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = \n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = \n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
