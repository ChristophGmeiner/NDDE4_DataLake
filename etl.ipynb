{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Explanation of ETL Steps\n",
    "\n",
    "First import and install all necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /opt/conda/lib/python3.6/site-packages (from s3fs) (1.14.9)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from s3fs) (0.6.2)\n",
      "Requirement already satisfied: boto3>=1.9.91 in /opt/conda/lib/python3.6/site-packages (from s3fs) (1.11.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.6.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.9.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.14)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /opt/conda/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.22)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3>=1.9.91->s3fs) (0.3.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import gc\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Get necessary credentials for accessing AWS S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Create Spark session and increase broadcast timeout. The last step depends on the size of the cluster / machine, which is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "spark.conf.set(\"spark.sql.broadcastTimeout\",  900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Empty target AWS S3 bucket in advance.<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "s3r = boto3.resource(\"s3\")\n",
    "bucket = s3r.Bucket(\"christophndde4\").objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Load and transform log data. Afterwards based on this dataframe, the user and time table is built. After that the 2 dimension dataframes are written to parguet files in another AWS S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.16 ms, sys: 0 ns, total: 9.16 ms\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sdf = spark.read.format(\"json\").load(\"s3a://udacity-dend/log_data/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.filter(F.col(\"page\")==\"NextSong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "userdf = sdf.select(F.col(\"userId\").alias(\"user_id\"), \n",
    "                 F.col(\"firstname\").alias(\"first_name\"), \n",
    "                 F.col(\"lastname\").alias(\"last_name\"), \n",
    "                 \"gender\", \"level\").distinct()\\\n",
    "         .orderBy(\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.8 ms, sys: 24.5 ms, total: 107 ms\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "userdf.write.parquet(\"s3a://christophndde4/user_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"timestamp\", F.expr(\"cast(ts / 1000 as timestamp)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"datetime\", F.expr(\"cast(timestamp as date)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tdf = sdf.select(F.col(\"timestamp\").alias(\"start_time\"))\\\n",
    "         .withColumn(\"hour\", F.hour(\"start_time\"))\\\n",
    "         .withColumn(\"day\", F.dayofmonth(\"start_time\"))\\\n",
    "         .withColumn(\"week\", F.weekofyear(\"start_time\"))\\\n",
    "         .withColumn(\"month\", F.month(\"start_time\"))\\\n",
    "         .withColumn(\"year\", F.year(\"start_time\"))\\\n",
    "         .withColumn(\"weekday\", F.dayofweek(\"start_time\"))\\\n",
    "         .orderBy(\"start_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 44.8 ms, total: 213 ms\n",
      "Wall time: 22min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tdf.write.partitionBy(\"year\", \"month\").parquet(\"s3a://christophndde4/time_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Load song data, create song, artist and songplay (fact) dataframe and write these two back to parquet files in another AWS S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 38.6 ms, total: 162 ms\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "songstage_df = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/*/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songsdf = songstage_df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \n",
    "                              \"duration\")\\\n",
    "                      .orderBy(F.col(\"song_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 193 ms, sys: 52.2 ms, total: 245 ms\n",
      "Wall time: 26min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "songs_df.write.partitionBy(\"year\", \"artist_id\").parquet(output_data + \"song_table.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artistdf = songstage_df.select(\"artist_id\", \n",
    "                               F.col(\"artist_name\").alias(\"name\"),\n",
    "                               F.col(\"artist_location\").alias(\"location\"),\n",
    "                               F.col(\"artist_latitude\").alias(\"latitude\"),\n",
    "                               F.col(\"artist_longitude\").alias(\"longitude\"))\\\n",
    "                       .distinct().orderBy(F.col(\"artist_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 54 ms, total: 324 ms\n",
      "Wall time: 24min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "artistdf.write.parquet(\"s3a://christophndde4/artist_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\"songplay_id\", F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songplay_df = sdf.join(songstage_df, \n",
    "                       (songstage_df.artist_name == sdf.artist) &\n",
    "                       (songstage_df.title == sdf.song),\n",
    "                       how=\"left\")\\\n",
    "                 .select(sdf[\"songplay_id\"],\n",
    "                         sdf[\"timestamp\"].alias(\"start_time\"),\n",
    "                         sdf[\"userId\"].alias(\"user_id\"),\n",
    "                         sdf[\"level\"],\n",
    "                         songstage_df[\"song_id\"],\n",
    "                         songstage_df[\"artist_id\"],\n",
    "                         sdf[\"sessionId\"].alias(\"session_id\"),\n",
    "                         songstage_df[\"artist_location\"].alias(\"location\"),\n",
    "                         sdf[\"userAgent\"].alias(\"user_agent\"))\\\n",
    "                 .filter(songstage_df[\"song_id\"].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 20.5 ms, total: 76.5 ms\n",
      "Wall time: 7min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "songplay_df.write.partitionBy(F.year(\"start_time\"), F.month(\"start_time\").parquet(\"s3a://christophndde4/songplay_table/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
